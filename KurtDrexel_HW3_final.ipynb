{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#by Kurt Drexel, April 2024\n",
    "#call this program using: python hw3.py inputdirectoryname outputdirectoryname\n",
    "#Origional code by Nia Klender and Kurt Drexel, February 2024\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "#downloads needed to get NLTK to work correctly:\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "fullstart = time.perf_counter() #for measuring the time the program takes to run\n",
    "\n",
    "#stop word list\n",
    "stopwords = [\"a\",\"about\",\"above\",\"according\",\"across\",\"actually\",\"adj\",\"after\",\"afterwards\",\"again\",\"against\",\"all\",\n",
    "             \"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\n",
    "             \"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anywhere\",\"are\",\"area\",\"areas\",\"aren't\",\"around\",\"as\",\"ask\",\"asked\",\n",
    "             \"asking\",\"asks\",\"at\",\"away\",\"b\",\"back\",\"backed\",\"backing\",\"backs\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\n",
    "             \"becoming\",\"been\",\"before\",\"beforehand\",\"began\",\"begin\",\"beginning\",\"behind\",\"being\",\"beings\",\"below\",\"beside\",\n",
    "             \"besides\",\"best\",\"better\",\"between\",\"beyond\",\"big\",\"billion\",\"both\",\"but\",\"by\",\"c\",\"came\",\"can\",\"can't\",\n",
    "             \"cannot\",\"caption\",\"case\",\"cases\",\"certain\",\"certainly\",\"clear\",\"clearly\",\"co\",\"come\",\"could\",\"couldn't\",\n",
    "             \"d\",\"did\",\"didn't\",\"differ\",\"different\",\"differently\",\"do\",\"does\",\"doesn't\",\"don't\",\"done\",\"down\",\"downed\",\n",
    "             \"downing\",\"downs\",\"during\",\"e\",\"each\",\"early\",\"eg\",\"eight\",\"eighty\",\"either\",\"else\",\"elsewhere\",\"end\",\"ended\",\n",
    "             \"ending\",\"ends\",\"enough\",\"etc\",\"even\",\"evenly\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\n",
    "             \"except\",\"f\",\"face\",\"faces\",\"fact\",\"facts\",\"far\",\"felt\",\"few\",\"fifty\",\"find\",\"finds\",\"first\",\"five\",\"for\",\n",
    "             \"former\",\"formerly\",\"forty\",\"found\",\"four\",\"from\",\"further\",\"furthered\",\"furthering\",\"furthers\",\"g\",\"gave\",\n",
    "             \"general\",\"generally\",\"get\",\"gets\",\"give\",\"given\",\"gives\",\"go\",\"going\",\"good\",\"goods\",\"got\",\"great\",\"greater\",\n",
    "             \"greatest\",\"group\",\"grouped\",\"grouping\",\"groups\",\"h\",\"had\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\n",
    "             \"he'd\",\"he'll\",\"he's\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\n",
    "             \"high\",\"higher\",\"highest\",\"him\",\"himself\",\"his\",\"how\",\"however\",\"hundred\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\n",
    "             \"if\",\"important\",\"in\",\"inc\",\"indeed\",\"instead\",\"interest\",\"interested\",\"interesting\",\"interests\",\"into\",\"is\",\n",
    "             \"isn't\",\"it\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"l\",\"large\",\"largely\",\"last\",\"later\",\"latest\",\"latter\",\n",
    "             \"latterly\",\"least\",\"less\",\"let\",\"let's\",\"lets\",\"like\",\"likely\",\"long\",\"longer\",\"longest\",\"ltd\",\"m\",\"made\",\n",
    "             \"make\",\"makes\",\"making\",\"man\",\"many\",\"may\",\"maybe\",\"me\",\"meantime\",\"meanwhile\",\"member\",\"members\",\"men\",\n",
    "             \"might\",\"million\",\"miss\",\"more\",\"moreover\",\"most\",\"mostly\",\"mr\",\"mrs\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"namely\",\n",
    "             \"necessary\",\"need\",\"needed\",\"needing\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"newer\",\"newest\",\"next\",\n",
    "             \"nine\",\"ninety\",\"no\",\"nobody\",\"non\",\"none\",\"nonetheless\",\"noone\",\"nor\",\"not\",\"nothing\",\"now\",\"nowhere\",\n",
    "             \"number\",\"numbers\",\"o\",\"of\",\"off\",\"often\",\"old\",\"older\",\"oldest\",\"on\",\"once\",\"one\",\"one's\",\"only\",\"onto\",\n",
    "             \"open\",\"opened\",\"opens\",\"or\",\"order\",\"ordered\",\"ordering\",\"orders\",\"other\",\"others\",\"otherwise\",\"our\",\"ours\",\n",
    "             \"ourselves\",\"out\",\"over\",\"overall\",\"own\",\"p\",\"part\",\"parted\",\"parting\",\"parts\",\"per\",\"perhaps\",\"place\",\n",
    "             \"places\",\"point\",\"pointed\",\"pointing\",\"points\",\"possible\",\"present\",\"presented\",\"presenting\",\"presents\",\n",
    "             \"problem\",\"problems\",\"put\",\"puts\",\"q\",\"quite\",\"r\",\"rather\",\"really\",\"recent\",\"recently\",\"right\",\"room\",\"rooms\",\n",
    "             \"s\",\"said\",\"same\",\"saw\",\"say\",\"says\",\"second\",\"seconds\",\"see\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seven\",\"seventy\",\n",
    "             \"several\",\"she\",\"she'd\",\"she'll\",\"she's\",\"should\",\"shouldn't\",\"show\",\"showed\",\"showing\",\"shows\",\"sides\",\"since\",\n",
    "             \"six\",\"sixty\",\"small\",\"smaller\",\"smallest\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\n",
    "             \"sometimes\",\"somewhere\",\"state\",\"states\",\"still\",\"stop\",\"such\",\"sure\",\"t\",\"take\",\"taken\",\"taking\",\"ten\",\"than\",\n",
    "             \"that\",\"that'll\",\"that's\",\"that've\",\"the\",\"their\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there'd\",\n",
    "             \"there'll\",\"there're\",\"there's\",\"there've\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"thereupon\",\"these\",\n",
    "             \"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"thing\",\"things\",\"think\",\"thinks\",\"thirty\",\"this\",\"those\",\"though\",\n",
    "             \"thought\",\"thoughts\",\"thousand\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"today\",\"together\",\"too\",\n",
    "             \"took\",\"toward\",\"towards\",\"trillion\",\"turn\",\"turned\",\"turning\",\"turns\",\"twenty\",\"two\",\"u\",\"under\",\"unless\",\n",
    "             \"unlike\",\"unlikely\",\"until\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"uses\",\"using\",\"v\",\"very\",\"via\",\"w\",\"want\",\"wanted\",\n",
    "             \"wanting\",\"wants\",\"was\",\"wasn't\",\"way\",\"ways\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"well\",\"wells\",\"were\",\n",
    "             \"weren't\",\"what\",\"what'll\",\"what's\",\"what've\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who'd\",\"who'll\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whomever\",\"whose\",\"why\",\"will\",\"with\",\"within\",\"without\",\"won't\",\"work\",\"worked\",\"working\",\"works\",\"would\",\"wouldn't\",\"x\",\"y\",\"year\",\"years\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"young\",\"younger\",\"youngest\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\"]\n",
    "#This stopword list could have been iterated through by the file but i decided to transfer it to an array.\n",
    "#I decided to do it this way because it would keep with the way me and my partner had done it during hw1.\n",
    "#for simplicity allowing me to simply paste the new stoplist inside the program and run it\n",
    "\n",
    "tokens = {}#dictionary to store all tokens and the number of times they occur\n",
    "#Occurances = defaultdict(lambda: defaultdict(int))\n",
    "Occurances = {}#defaultdict(lambda: defaultdict(int))\n",
    "#defaultdict will help to account for new words detected in dict adding them and setting the counts accordingly \n",
    "ProcessedTok = {}#hold processed tokens\n",
    "Titles =[]#list of filenames without html\n",
    "Weights = {}#dictionary of tf*idf weights\n",
    "TermOccurances = defaultdict(lambda: defaultdict(int))\n",
    "#two forms of input one for notebooks the other for normal python programs\n",
    "#directory_path = sys.argv[1] #takes command line argument for the directory where the input files are located\n",
    "directory_path = input() #used to get input in jupyter\n",
    "#output_directory = sys.argv[2] #takes command line argument for the output directory (does not need to exist already)\n",
    "output_directory = input() #used to get input in jupyter\n",
    "\n",
    "if not os.path.exists(output_directory):#if output directory does not exist, create it\n",
    "    os.makedirs(output_directory)\n",
    "words = [] #array to hold words gotten from each document\n",
    "for filename in os.listdir(directory_path):#iterate through files\n",
    "    starttime = time.perf_counter()\n",
    "    DocNum = filename.rsplit('.')[0]  # get begining of filename without html\n",
    "    Titles.append(DocNum)\n",
    "    words.clear()#clear word array to start a new document\n",
    "    f = os.path.join(directory_path, filename)\n",
    "    if os.path.isfile(f): #check if it's a file\n",
    "        with open(f, 'r', encoding = 'latin-1') as f:#open file to read\n",
    "            for line in f: #for each line in the file\n",
    "                #tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                tokenizer = RegexpTokenizer(r'<|>|\\'|\\w+') #remove punctuation from tokens, except < and >\n",
    "                                                        #these will be used to identify and remove html syntax\n",
    "                newtokens = tokenizer.tokenize(line) #tokenize line\n",
    "                words.extend(newtokens) #add tokenized words to the word array\n",
    "            flag = 0\n",
    "            discard_pile = []\n",
    "            for i in range(len(words)):\n",
    "                words[i] = words[i].lower()\n",
    "                if words[i] == \"<\": #detect if it is part of html syntax\n",
    "                    flag += 1\n",
    "                    discard_pile.insert(0,i)\n",
    "                elif words[i] == \">\": #detect end of html syntax\n",
    "                    flag -= 1\n",
    "                    discard_pile.insert(0,i)\n",
    "                elif flag > 0: #if flag is up discard current tokenized word\n",
    "                    discard_pile.insert(0, i)\n",
    "                elif len(words[i]) == 1:#words of length 1 are stopped by stoplist but this will prevent single digits as well\n",
    "                    discard_pile.insert(0,i)\n",
    "            for i in discard_pile: #iterate through discard pile and remove them all from the list of words\n",
    "                words.pop(i) #ordered by indices in reverse, so it doesn't get mixed up when removing\n",
    "\n",
    "            for stopword in stopwords: #iterate through stopwords\n",
    "                while stopword in words:\n",
    "                    words.remove(stopword) #stopword found in words array, remove stopword\n",
    "        #print(len(words))\n",
    "        ProcessedTok[DocNum] = copy.deepcopy(words)\n",
    "        #this needs to be a deepcopy as a normal copy will become useless after words is cleared when\n",
    "        #iterating to the next file\n",
    "        for token in words:#iterate through words\n",
    "            if token in tokens:\n",
    "                tokens[token] += 1#if token occurs more then once increment count\n",
    "            else:\n",
    "                tokens[token] = 1#first occurance of token\n",
    "        for token in words:\n",
    "            if not token in Occurances:\n",
    "                Occurances[token] = []\n",
    "                Occurances[token].append(DocNum)\n",
    "            else:\n",
    "                if not DocNum in Occurances[token]:\n",
    "                    Occurances[token].append(DocNum)\n",
    "                    #TermOccurances[token][DocNum] += 1#termoccurances will hold the term and each document it appears in\n",
    "\n",
    "#delete un-neccessary tokens\n",
    "keys = list(tokens.keys())\n",
    "for i in range(len(keys)):#iterate through words\n",
    "    if tokens[keys[i]] == 1:#if word occurs once\n",
    "        del tokens[keys[i]]  # delete once occuring word\n",
    "        Pos = Occurances[keys[i]][0]  # position of deleted word\n",
    "        ProcessedTok[Pos].remove(keys[i])  # remove from dictonary\n",
    "        del Occurances[keys[i]]  # delete in occurances\n",
    "        i -= 1\n",
    "\n",
    "#tf*idf and normalization\n",
    "Sum =0#initialize to 0\n",
    "for DocNum in Titles: #iterate through document titles\n",
    "    #print(DocNum)\n",
    "    Sum = 0#reset sum for each new document\n",
    "    Weights[DocNum] = {}\n",
    "    for token in Occurances:  # iterate through tokens in document\n",
    "        weight = 0#intialize new weight for each term in document\n",
    "        if DocNum in Occurances[token]:#iterate through tokenized words\n",
    "            df = len(Occurances[token])  # of documents the token appears in or document frequency (tf)####always =1\n",
    "            tf = ProcessedTok[DocNum].count(token)  # of times t occurs in doc d or term frequency (idf)\n",
    "            weight = math.log(1 + tf) * math.log10(503 / df)#calculate TF*IDF\n",
    "            Sum += weight*weight #square w\n",
    "            Weights[DocNum][token] = weight  # set token frequency\n",
    "    sqrt_sum = math.sqrt(Sum)\n",
    "    for token in Weights[DocNum]:\n",
    "        Weights[DocNum][token] /=sqrt_sum#calculate new frequency\n",
    "#Weights = {docnum1={token1=2,token2=5}, docnum2={token1=9,token2=4}}\n",
    "\n",
    "Occurances = OrderedDict(sorted(Occurances.items()))#order the current dictionary #alphabetical\n",
    "\n",
    "#create dictionary and postings files\n",
    "with open(os.path.join(output_directory, 'dictionary.txt'), 'w') as dict_file:\n",
    "    with open(os.path.join(output_directory, 'postings.txt'), 'w') as postings_file:\n",
    "        curr_line = 1\n",
    "        # Iterate through terms and write entries to dictionary and postings\n",
    "        for term, document in Occurances.items():#iterate through terms\n",
    "            num_docs_containing_term = len(document)  # Number of documents containing the term\n",
    "            # Write postings for the term\n",
    "            for doc in document:#iterate through documents term occurs in\n",
    "                #postings_file.write(f\"{curr_doc},{doc},{Weights[doc][term]}\\n\")\n",
    "                postings_file.write(f\"{doc},{Weights[doc][term]}\\n\")#write to postings file\n",
    "            dict_file.write(f\"{term}\\n{num_docs_containing_term}\\n{curr_line}\\n\")#write to dict file\n",
    "            curr_line = curr_line+num_docs_containing_term #update current line\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
